# Step 000021: Metadata Extraction from Tool Definitions

## Overview
Implement sophisticated metadata extraction capabilities that can parse tool definitions, docstrings, and schemas to create rich, searchable metadata for MCP tools. This enables intelligent tool discovery based on functionality, parameters, and usage patterns.

## Context
Building on the tool discovery framework from Step 000020, this step focuses on extracting comprehensive metadata from discovered tools. This includes parsing JSON schemas, analyzing docstrings, inferring usage patterns, and extracting practical examples that can be used for tool recommendation and comparison.

## Implementation Tasks

### 1. Schema Analysis Engine
- Parse JSON schemas from tool parameter definitions
- Extract parameter types, constraints, and validation rules
- Identify required vs optional parameters
- Analyze parameter relationships and dependencies

### 2. Docstring and Documentation Parser
- Parse structured docstrings (Google, Sphinx, NumPy formats)
- Extract parameter descriptions and usage notes
- Identify examples and code snippets
- Extract return type and error condition information

### 3. Usage Pattern Recognition
- Infer common usage patterns from tool structure
- Identify tool complexity levels based on parameter counts
- Extract best practices and common pitfalls from documentation
- Generate usage confidence scores

### 4. Example Extraction and Generation
- Extract usage examples from documentation and comments
- Generate synthetic examples for common use cases
- Create parameter combination examples
- Build example libraries for each tool

## Success Criteria
- [ ] Comprehensive metadata extracted for all system tools
- [ ] Parameter schemas fully parsed with type information and constraints
- [ ] Docstring analysis provides clear parameter descriptions and examples
- [ ] Usage patterns identified and categorized by complexity
- [ ] Tool examples extracted or generated for common use cases
- [ ] Metadata accuracy validated against manual tool inspection

## Files to Create/Modify
- `mcp_metadata_extractor.py` - Core metadata extraction engine
- `schema_analyzer.py` - JSON schema parsing and analysis
- `docstring_parser.py` - Documentation parsing utilities
- `usage_pattern_detector.py` - Pattern recognition and inference
- `example_generator.py` - Example extraction and generation
- `tests/test_metadata_extraction.py` - Comprehensive extraction testing

## Implementation Details

### MCPMetadataExtractor Class Structure
```python
class MCPMetadataExtractor:
    """Extract rich metadata from MCP tool definitions"""
    
    def __init__(self, schema_analyzer: SchemaAnalyzer, docstring_parser: DocstringParser):
        self.schema_analyzer = schema_analyzer
        self.docstring_parser = docstring_parser
        self.pattern_detector = UsagePatternDetector()
        self.example_generator = ExampleGenerator()
    
    def extract_from_tool_definition(self, tool_def: Dict[str, Any]) -> MCPToolMetadata:
        """Extract comprehensive metadata from tool definition"""
        # Combine schema analysis, docstring parsing, and pattern detection
        
    def analyze_parameter_schema(self, schema: Dict[str, Any]) -> List[ParameterAnalysis]:
        """Deep analysis of parameter schemas"""
        # Parse JSON schema structure
        # Extract types, constraints, defaults
        # Identify parameter relationships
        
    def parse_tool_documentation(self, docstring: str) -> DocumentationAnalysis:
        """Extract structured information from tool documentation"""
        # Parse structured docstrings
        # Extract examples and usage notes
        # Identify best practices and warnings
        
    def infer_usage_patterns(self, tool_metadata: Dict[str, Any]) -> List[UsagePattern]:
        """Infer common usage patterns from tool structure"""
        # Analyze parameter complexity
        # Identify common parameter combinations
        # Generate complexity scores
```

### Schema Analysis Components
```python
@dataclass
class ParameterAnalysis:
    """Detailed analysis of a tool parameter"""
    name: str
    type: str
    required: bool
    description: str
    constraints: Dict[str, Any]  # min/max, enum values, patterns
    default_value: Optional[Any]
    examples: List[Any]
    complexity_score: float  # 0.0 = simple, 1.0 = complex
    
@dataclass
class UsagePattern:
    """Identified usage pattern for a tool"""
    pattern_name: str
    description: str
    parameter_combination: List[str]
    use_case: str
    complexity_level: str  # 'basic', 'intermediate', 'advanced'
    example_code: str
    success_probability: float
```

### Documentation Analysis
```python
class DocstringParser:
    """Parse and analyze tool documentation"""
    
    def parse_structured_docstring(self, docstring: str) -> DocstringAnalysis:
        """Parse Google/Sphinx/NumPy style docstrings"""
        # Extract sections: description, args, returns, examples
        # Parse parameter descriptions
        # Extract usage examples and notes
        
    def extract_examples(self, docstring: str) -> List[ToolExample]:
        """Extract code examples from documentation"""
        # Find code blocks in docstrings
        # Parse example parameters and expected outputs
        # Create structured example objects
        
    def identify_best_practices(self, docstring: str) -> List[str]:
        """Extract best practices and recommendations"""
        # Look for warning patterns
        # Extract performance notes
        # Identify common pitfalls
```

### Usage Pattern Detection
```python
class UsagePatternDetector:
    """Detect and classify tool usage patterns"""
    
    def analyze_parameter_complexity(self, parameters: List[ParameterAnalysis]) -> ComplexityAnalysis:
        """Analyze tool complexity based on parameters"""
        # Count required vs optional parameters
        # Analyze parameter types and constraints
        # Generate complexity score
        
    def identify_common_patterns(self, tool_metadata: MCPToolMetadata) -> List[UsagePattern]:
        """Identify common usage patterns for the tool"""
        # Analyze parameter relationships
        # Generate typical use case scenarios
        # Create pattern templates
        
    def generate_complexity_score(self, tool: MCPToolMetadata) -> float:
        """Generate overall tool complexity score"""
        # Factor in parameter count and types
        # Consider schema complexity
        # Account for example availability
```

## Metadata Extraction Process
1. **Tool Definition Analysis**: Parse raw tool definitions and schemas
2. **Schema Deep-Dive**: Analyze JSON schemas for parameters and constraints
3. **Documentation Parsing**: Extract structured information from docstrings
4. **Pattern Recognition**: Identify usage patterns and complexity levels
5. **Example Processing**: Extract or generate usage examples
6. **Metadata Assembly**: Combine all analysis into comprehensive metadata
7. **Validation**: Verify extracted metadata accuracy and completeness

## Tool Categories and Complexity Mapping
```python
COMPLEXITY_FACTORS = {
    'parameter_count': {'weight': 0.3, 'threshold_simple': 3, 'threshold_complex': 8},
    'required_parameters': {'weight': 0.2, 'threshold_simple': 1, 'threshold_complex': 5},
    'schema_depth': {'weight': 0.2, 'threshold_simple': 2, 'threshold_complex': 4},
    'type_complexity': {'weight': 0.2, 'multiplier_object': 2.0, 'multiplier_array': 1.5},
    'documentation_quality': {'weight': 0.1, 'has_examples': 0.8, 'detailed_descriptions': 1.0},
}

USAGE_PATTERNS = {
    'simple_file_operation': ['file_path', 'content?'],
    'search_operation': ['pattern', 'path?', 'options?'],
    'execution_task': ['command', 'timeout?', 'description?'],
    'data_transformation': ['input', 'transformation_params', 'output_format?'],
}
```

## Dependencies
- Step 000019 (Database Schema) must be completed
- Step 000020 (Tool Discovery) must be completed
- Requires JSON schema validation libraries
- Depends on docstring parsing libraries (docstring_parser, sphinx)

## Technical Notes
- Use industry-standard docstring parsing libraries
- Implement caching for expensive metadata extraction operations
- Handle malformed schemas and documentation gracefully
- Create confidence scores for extracted metadata
- Support multiple documentation formats and conventions
- Generate machine-readable metadata alongside human-readable descriptions

## Estimated Effort
8-10 hours

## Risk Mitigation
- **Documentation Quality Variance**: Implement fallback extraction methods for poorly documented tools
- **Schema Complexity**: Start with simple schema patterns, gradually handle more complex cases
- **Pattern Recognition Accuracy**: Use rule-based approaches with machine learning enhancement later
- **Performance Impact**: Implement caching and lazy loading for metadata extraction
- **Metadata Completeness**: Provide default values and synthetic examples when original data is insufficient