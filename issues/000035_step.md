# Step 000035: Comprehensive Testing Framework for Tool Search

## Overview
Implement a comprehensive testing framework specifically for the MCP Tool Search System, covering all components from database operations through MCP tool responses. This ensures system reliability, accuracy, and performance before full deployment.

## Context
Building on the complete tool search system implementation from Steps 000019-000031, this step creates thorough testing infrastructure that validates every aspect of the system. This includes unit tests, integration tests, performance tests, and end-to-end validation scenarios.

## Implementation Tasks

### 1. Core Component Testing
- Create comprehensive unit tests for all search and recommendation components
- Build integration tests for database operations and tool cataloging
- Implement performance tests for search response times and scalability
- Add edge case testing for error conditions and boundary scenarios

### 2. MCP Tool Testing
- Create comprehensive testing for all MCP tool functions
- Build response format validation testing
- Implement Claude Code integration testing with realistic scenarios
- Add load testing for concurrent MCP tool usage

### 3. Search Accuracy Testing
- Implement search relevance testing with known query-result pairs
- Create recommendation accuracy validation with expert-labeled datasets
- Build comparison accuracy testing for tool relationship detection
- Add confidence score calibration testing

### 4. End-to-End Workflow Testing
- Create complete workflow tests from tool discovery through selection
- Build multi-tool sequence testing for complex scenarios
- Implement user journey testing with realistic development tasks
- Add regression testing to prevent feature degradation

## Success Criteria
- [ ] >95% test coverage across all tool search system components
- [ ] All search accuracy tests show >90% relevance for well-formed queries
- [ ] Performance tests confirm <2 second response times for typical operations
- [ ] Integration tests validate seamless Claude Code interaction
- [ ] Load tests confirm system stability under concurrent usage
- [ ] Regression test suite prevents feature degradation

## Files to Create/Modify
- `tests/tool_search/` - New directory for tool search specific tests
- `tests/tool_search/test_search_engine.py` - Search engine testing
- `tests/tool_search/test_recommendation_engine.py` - Recommendation testing
- `tests/tool_search/test_mcp_tools.py` - MCP tool integration testing
- `tests/tool_search/test_search_accuracy.py` - Search accuracy validation
- `tests/tool_search/test_performance.py` - Performance and load testing
- `conftest.py` - Enhanced fixtures for tool search testing

## Implementation Details

### Search Engine Testing
```python
class TestToolSearchEngine:
    """Comprehensive testing for tool search functionality"""
    
    @pytest.fixture
    def search_engine(self, db_manager, embedding_generator):
        """Create configured search engine for testing"""
        return MCPToolSearchEngine(
            db_manager=db_manager,
            embedding_generator=embedding_generator,
            query_processor=ToolQueryProcessor()
        )
        
    @pytest.fixture
    def sample_tools(self, db_manager):
        """Create sample tool dataset for testing"""
        tools = [
            create_test_tool("bash", "Execute shell commands", ["command", "timeout"]),
            create_test_tool("read", "Read file contents", ["file_path", "offset", "limit"]),
            create_test_tool("write", "Write file contents", ["file_path", "content"]),
            create_test_tool("search_code", "Search code semantically", ["query", "max_results"])
        ]
        
        # Store in database
        for tool in tools:
            db_manager.store_tool(tool)
            
        return tools
        
    def test_semantic_search_accuracy(self, search_engine, sample_tools):
        """Test semantic search accuracy with known queries"""
        
        test_cases = [
            {
                'query': 'execute shell commands with timeout',
                'expected_top_result': 'bash',
                'min_similarity': 0.8
            },
            {
                'query': 'read file contents from disk',
                'expected_top_result': 'read', 
                'min_similarity': 0.7
            },
            {
                'query': 'find code using natural language',
                'expected_top_result': 'search_code',
                'min_similarity': 0.8
            }
        ]
        
        for test_case in test_cases:
            results = search_engine.search_by_functionality(
                query=test_case['query'],
                k=5
            )
            
            # Validate top result
            assert len(results) > 0
            assert results[0].tool_id == test_case['expected_top_result']
            assert results[0].similarity_score >= test_case['min_similarity']
            
    def test_parameter_aware_search(self, search_engine, sample_tools):
        """Test parameter-aware search functionality"""
        
        # Search for tools with specific parameters
        results = search_engine.search_by_parameters(
            input_types=['string'],
            required_parameters=['file_path']
        )
        
        # Should find read and write tools
        tool_ids = [r.tool_id for r in results]
        assert 'read' in tool_ids
        assert 'write' in tool_ids
        
        # Verify parameter matching explanations
        for result in results:
            assert 'file_path' in [p.name for p in result.parameters]
            assert len(result.match_reasons) > 0
            
    def test_search_performance(self, search_engine, large_tool_dataset):
        """Test search performance with large dataset"""
        
        start_time = time.time()
        
        results = search_engine.search_by_functionality(
            query="file processing tools",
            k=20
        )
        
        execution_time = time.time() - start_time
        
        # Performance requirements
        assert execution_time < 2.0  # Must complete within 2 seconds
        assert len(results) > 0
        assert all(r.similarity_score > 0.1 for r in results)
        
    def test_hybrid_search_combination(self, search_engine, sample_tools):
        """Test hybrid search combining semantic and keyword matching"""
        
        results = search_engine.search_hybrid(
            query="bash shell command execution",
            semantic_weight=0.7,
            keyword_weight=0.3
        )
        
        # Should strongly favor bash tool
        assert results[0].tool_id == 'bash'
        assert results[0].relevance_score > 0.8
        
        # Verify hybrid scoring worked
        assert hasattr(results[0], 'semantic_score')
        assert hasattr(results[0], 'keyword_score')

@pytest.fixture
def large_tool_dataset(db_manager):
    """Create large tool dataset for performance testing"""
    tools = []
    
    # Create 100 diverse tools for performance testing
    categories = ['file_ops', 'web', 'analysis', 'development', 'execution']
    
    for i in range(100):
        category = categories[i % len(categories)]
        tool = create_test_tool(
            f"test_tool_{i}",
            f"Test tool for {category} operations number {i}",
            [f"param_{j}" for j in range(random.randint(2, 8))],
            category=category
        )
        tools.append(tool)
        db_manager.store_tool(tool)
        
    return tools
```

### Recommendation Testing
```python
class TestRecommendationEngine:
    """Testing for tool recommendation functionality"""
    
    @pytest.fixture
    def recommendation_engine(self, search_engine, task_analyzer):
        """Create configured recommendation engine"""
        return ToolRecommendationEngine(
            tool_search_engine=search_engine,
            parameter_search_engine=ParameterSearchEngine(),
            task_analyzer=task_analyzer,
            context_analyzer=ContextAnalyzer()
        )
        
    def test_task_based_recommendations(self, recommendation_engine):
        """Test recommendations for specific development tasks"""
        
        test_tasks = [
            {
                'description': 'read configuration file and parse JSON data',
                'expected_tools': ['read'],
                'expected_reasoning': ['file reading', 'configuration']
            },
            {
                'description': 'execute tests with timeout and capture output',
                'expected_tools': ['bash'],
                'expected_reasoning': ['command execution', 'timeout']
            }
        ]
        
        for task in test_tasks:
            recommendations = recommendation_engine.recommend_for_task(
                task_description=task['description'],
                max_recommendations=5
            )
            
            # Validate recommendations
            assert len(recommendations) > 0
            top_recommendation = recommendations[0]
            
            assert top_recommendation.tool.tool_id in task['expected_tools']
            assert top_recommendation.recommendation_score > 0.7
            assert len(top_recommendation.recommendation_reasons) > 0
            
            # Check reasoning contains expected terms
            reasoning_text = ' '.join(top_recommendation.recommendation_reasons).lower()
            assert any(term in reasoning_text for term in task['expected_reasoning'])
            
    def test_recommendation_explanations(self, recommendation_engine):
        """Test quality of recommendation explanations"""
        
        recommendations = recommendation_engine.recommend_for_task(
            task_description="process CSV files and generate reports",
            explain_reasoning=True
        )
        
        for rec in recommendations:
            # Each recommendation should have clear explanations
            assert len(rec.recommendation_reasons) >= 2
            assert len(rec.usage_guidance) > 0
            assert rec.when_to_use is not None
            
            # Explanations should be meaningful (not generic)
            reasons_text = ' '.join(rec.recommendation_reasons)
            assert len(reasons_text) > 50  # Substantial explanation
            assert 'recommended' not in reasons_text.lower()  # Avoid circular reasoning
            
    def test_context_aware_recommendations(self, recommendation_engine):
        """Test context-sensitive recommendations"""
        
        base_task = "read and process data files"
        
        # Test with performance-critical context
        perf_recs = recommendation_engine.recommend_for_task(
            task_description=base_task,
            context=create_task_context(
                context_description="performance critical large files",
                complexity_preference="powerful"
            )
        )
        
        # Test with beginner-friendly context  
        beginner_recs = recommendation_engine.recommend_for_task(
            task_description=base_task,
            context=create_task_context(
                complexity_preference="simple"
            )
        )
        
        # Should get different recommendations based on context
        perf_tool = perf_recs[0].tool.tool_id if perf_recs else None
        beginner_tool = beginner_recs[0].tool.tool_id if beginner_recs else None
        
        # May be same tool but reasoning should differ
        if perf_tool == beginner_tool:
            perf_reasoning = ' '.join(perf_recs[0].recommendation_reasons)
            beginner_reasoning = ' '.join(beginner_recs[0].recommendation_reasons)
            assert perf_reasoning != beginner_reasoning
```

### MCP Tool Integration Testing
```python
class TestMCPToolIntegration:
    """Testing for MCP tool functions and responses"""
    
    @pytest.fixture
    def mcp_server(self, db_manager, tool_search_system):
        """Create configured MCP server for testing"""
        return create_test_mcp_server(
            db_manager=db_manager,
            tool_search_system=tool_search_system
        )
        
    async def test_search_mcp_tools_response_format(self, mcp_server):
        """Test search_mcp_tools response format compliance"""
        
        response = await mcp_server.call_tool(
            'search_mcp_tools',
            {
                'query': 'file operations',
                'max_results': 5,
                'include_examples': True
            }
        )
        
        # Validate response structure
        assert response['success'] is True
        assert 'query' in response
        assert 'results' in response
        assert 'total_results' in response
        assert 'timestamp' in response
        assert 'version' in response
        
        # Validate results structure
        for result in response['results']:
            assert 'tool_id' in result
            assert 'name' in result
            assert 'description' in result
            assert 'similarity_score' in result
            assert 'confidence_level' in result
            
        # Validate JSON serialization
        json_str = json.dumps(response)
        assert len(json_str) > 0
        
    async def test_mcp_tool_error_handling(self, mcp_server):
        """Test MCP tool error handling and responses"""
        
        # Test invalid query
        response = await mcp_server.call_tool(
            'search_mcp_tools',
            {
                'query': '',  # Empty query
                'max_results': 5
            }
        )
        
        assert response['success'] is False
        assert 'error' in response
        assert 'message' in response['error']
        assert 'suggestions' in response['error']
        
        # Test invalid tool ID
        response = await mcp_server.call_tool(
            'get_tool_details',
            {
                'tool_id': 'nonexistent_tool'
            }
        )
        
        assert response['success'] is False
        assert 'not found' in response['error']['message'].lower()
        
    async def test_claude_code_integration(self, mcp_server):
        """Test integration with Claude Code usage patterns"""
        
        # Simulate Claude Code workflow
        workflow_steps = [
            # Step 1: Search for tools
            {
                'tool': 'search_mcp_tools',
                'params': {'query': 'file reading with error handling'}
            },
            # Step 2: Get details on top result
            {
                'tool': 'get_tool_details', 
                'params': {'tool_id': 'read', 'include_examples': True}
            },
            # Step 3: Get recommendations for specific task
            {
                'tool': 'recommend_tools_for_task',
                'params': {'task_description': 'safely read configuration files'}
            }
        ]
        
        responses = []
        for step in workflow_steps:
            response = await mcp_server.call_tool(step['tool'], step['params'])
            responses.append(response)
            
            # Each step should succeed
            assert response['success'] is True
            
        # Validate workflow coherence
        search_results = responses[0]['results']
        tool_details = responses[1]['tool_details']
        recommendations = responses[2]['recommendations']
        
        # Tool details should match search result
        top_search_tool = search_results[0]['tool_id']
        assert tool_details['tool_id'] == top_search_tool
        
        # Recommendations should be relevant
        rec_tool_ids = [r['tool']['tool_id'] for r in recommendations]
        assert top_search_tool in rec_tool_ids
```

### Search Accuracy Testing
```python
class TestSearchAccuracy:
    """Validate search accuracy against expert-labeled datasets"""
    
    @pytest.fixture
    def expert_labeled_dataset(self):
        """Load expert-labeled search queries and expected results"""
        return load_expert_dataset('tool_search_validation.json')
        
    def test_search_relevance_accuracy(self, search_engine, expert_labeled_dataset):
        """Test search relevance against expert judgments"""
        
        accuracy_scores = []
        
        for test_case in expert_labeled_dataset:
            query = test_case['query']
            expected_relevant_tools = set(test_case['relevant_tools'])
            
            # Perform search
            results = search_engine.search_by_functionality(query, k=10)
            returned_tools = set(r.tool_id for r in results)
            
            # Calculate precision and recall
            relevant_returned = expected_relevant_tools & returned_tools
            precision = len(relevant_returned) / len(returned_tools) if returned_tools else 0
            recall = len(relevant_returned) / len(expected_relevant_tools)
            
            # F1 score
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            accuracy_scores.append(f1)
            
        # Overall accuracy should be high
        avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)
        assert avg_accuracy >= 0.85  # 85% accuracy threshold
        
    def test_recommendation_ranking_quality(self, recommendation_engine, expert_labeled_dataset):
        """Test quality of recommendation rankings"""
        
        ndcg_scores = []
        
        for test_case in expert_labeled_dataset:
            if 'task_description' not in test_case:
                continue
                
            task = test_case['task_description']
            expected_ranking = test_case['expected_ranking']
            
            # Get recommendations
            recs = recommendation_engine.recommend_for_task(task, max_recommendations=len(expected_ranking))
            actual_ranking = [r.tool.tool_id for r in recs]
            
            # Calculate NDCG (Normalized Discounted Cumulative Gain)
            ndcg = calculate_ndcg(actual_ranking, expected_ranking)
            ndcg_scores.append(ndcg)
            
        avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)
        assert avg_ndcg >= 0.80  # 80% ranking quality threshold
```

### Performance Testing
```python
class TestPerformance:
    """Performance and load testing for tool search system"""
    
    def test_concurrent_search_performance(self, search_engine):
        """Test performance under concurrent search load"""
        
        async def perform_search(query_id):
            """Perform individual search operation"""
            start_time = time.time()
            results = search_engine.search_by_functionality(f"test query {query_id}")
            execution_time = time.time() - start_time
            return execution_time
            
        # Simulate 20 concurrent searches
        import asyncio
        async def run_concurrent_tests():
            tasks = [perform_search(i) for i in range(20)]
            execution_times = await asyncio.gather(*tasks)
            return execution_times
            
        execution_times = asyncio.run(run_concurrent_tests())
        
        # Validate performance under load
        avg_time = sum(execution_times) / len(execution_times)
        max_time = max(execution_times)
        
        assert avg_time < 2.0  # Average under 2 seconds
        assert max_time < 5.0  # No search takes more than 5 seconds
        assert len(execution_times) == 20  # All searches completed
        
    def test_database_scalability(self, db_manager):
        """Test database performance with large tool catalogs"""
        
        # Create large tool dataset
        create_large_tool_dataset(db_manager, num_tools=1000)
        
        # Test search performance
        start_time = time.time()
        results = db_manager.search_tools_by_embedding(
            embedding=[0.1] * 384,  # Sample embedding
            k=50
        )
        execution_time = time.time() - start_time
        
        assert execution_time < 3.0  # Should scale to 1000 tools
        assert len(results) == 50
```

### Test Utilities
```python
def create_test_tool(tool_id: str, 
                    description: str, 
                    parameters: List[str],
                    category: str = 'test') -> MCPTool:
    """Create test tool with specified parameters"""
    
    param_objects = [
        ParameterInfo(
            name=param,
            type='string',
            required=True,
            description=f"Description for {param}",
            default_value=None,
            schema={}
        ) 
        for param in parameters
    ]
    
    return MCPTool(
        id=tool_id,
        name=tool_id,
        description=description,
        tool_type='test',
        provider='test',
        category=category,
        parameters=param_objects,
        examples=[],
        metadata={}
    )

def calculate_ndcg(actual_ranking: List[str], expected_ranking: List[str]) -> float:
    """Calculate Normalized Discounted Cumulative Gain"""
    dcg = 0.0
    for i, tool_id in enumerate(actual_ranking):
        if tool_id in expected_ranking:
            relevance = len(expected_ranking) - expected_ranking.index(tool_id)
            dcg += relevance / math.log2(i + 2)
            
    # Calculate ideal DCG
    ideal_dcg = sum(
        (len(expected_ranking) - i) / math.log2(i + 2)
        for i in range(len(expected_ranking))
    )
    
    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0

async def create_test_mcp_server(db_manager, tool_search_system):
    """Create MCP server configured for testing"""
    server = FastMCP("test-tool-search")
    
    # Register test versions of MCP tools
    server.add_tool(search_mcp_tools)
    server.add_tool(get_tool_details)
    server.add_tool(recommend_tools_for_task)
    
    return server
```

## Test Data and Fixtures
Create comprehensive test datasets:

```python
# conftest.py additions
@pytest.fixture(scope="session")
def expert_dataset():
    """Expert-labeled dataset for validation"""
    return {
        "queries": [
            {
                "query": "execute shell commands safely",
                "relevant_tools": ["bash", "task"],
                "expected_ranking": ["bash", "task"],
                "task_description": "run system commands with timeout"
            }
            # ... more test cases
        ]
    }
```

## Dependencies
- All previous steps (000019-000031) must be completed
- Requires comprehensive test data and fixtures
- Depends on pytest framework and testing utilities
- Needs performance testing tools and load generation

## Technical Notes
- Create comprehensive test data covering edge cases and typical usage
- Implement test data generation for scalability testing
- Use fixtures to ensure test isolation and repeatability
- Create performance benchmarks for regression detection
- Implement test result analysis and reporting tools

## Estimated Effort
20-24 hours

## Risk Mitigation
- **Test Coverage**: Use coverage tools to ensure comprehensive testing
- **Test Data Quality**: Use expert-labeled datasets for accuracy validation  
- **Performance Regression**: Implement automated performance monitoring
- **Integration Issues**: Test with realistic Claude Code usage patterns
- **Test Maintenance**: Design tests to be maintainable as system evolves
- **False Positives**: Implement robust test oracles and validation logic